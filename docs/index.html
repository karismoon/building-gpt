<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ul.lst-kix_m1y38rtq2dr5-3{list-style-type:none}ul.lst-kix_m1y38rtq2dr5-4{list-style-type:none}.lst-kix_m1y38rtq2dr5-5>li:before{content:"\0025a0   "}ul.lst-kix_m1y38rtq2dr5-5{list-style-type:none}ul.lst-kix_m1y38rtq2dr5-6{list-style-type:none}.lst-kix_m1y38rtq2dr5-4>li:before{content:"\0025cb   "}ol.lst-kix_1mcqazeuqmqh-1.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-1 0}.lst-kix_1mcqazeuqmqh-3>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-3}ul.lst-kix_m1y38rtq2dr5-0{list-style-type:none}.lst-kix_1mcqazeuqmqh-8>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-8}ul.lst-kix_m1y38rtq2dr5-1{list-style-type:none}ul.lst-kix_m1y38rtq2dr5-2{list-style-type:none}ul.lst-kix_55rfy9bdzp8h-5{list-style-type:none}.lst-kix_55rfy9bdzp8h-1>li:before{content:"\0025cb   "}.lst-kix_55rfy9bdzp8h-2>li:before{content:"\0025a0   "}ul.lst-kix_55rfy9bdzp8h-6{list-style-type:none}ul.lst-kix_55rfy9bdzp8h-7{list-style-type:none}ul.lst-kix_55rfy9bdzp8h-8{list-style-type:none}.lst-kix_m1y38rtq2dr5-1>li:before{content:"\0025cb   "}.lst-kix_m1y38rtq2dr5-3>li:before{content:"\0025cf   "}ul.lst-kix_m1y38rtq2dr5-7{list-style-type:none}.lst-kix_55rfy9bdzp8h-0>li:before{content:"\0025cf   "}ul.lst-kix_m1y38rtq2dr5-8{list-style-type:none}.lst-kix_m1y38rtq2dr5-2>li:before{content:"\0025a0   "}ol.lst-kix_1mcqazeuqmqh-4.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-4 0}.lst-kix_1mcqazeuqmqh-8>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-8,lower-roman) ". "}.lst-kix_1mcqazeuqmqh-7>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-7}.lst-kix_1mcqazeuqmqh-7>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-7,lower-latin) ". "}.lst-kix_m1y38rtq2dr5-0>li:before{content:"\0025cf   "}ol.lst-kix_1mcqazeuqmqh-0.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-0 0}ol.lst-kix_1mcqazeuqmqh-7.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-7 0}.lst-kix_55rfy9bdzp8h-8>li:before{content:"\0025a0   "}.lst-kix_1mcqazeuqmqh-1>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-1}.lst-kix_55rfy9bdzp8h-3>li:before{content:"\0025cf   "}ol.lst-kix_1mcqazeuqmqh-3.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-3 0}.lst-kix_1mcqazeuqmqh-6>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-6}ol.lst-kix_1mcqazeuqmqh-0{list-style-type:none}.lst-kix_55rfy9bdzp8h-4>li:before{content:"\0025cb   "}.lst-kix_m1y38rtq2dr5-6>li:before{content:"\0025cf   "}.lst-kix_55rfy9bdzp8h-5>li:before{content:"\0025a0   "}.lst-kix_55rfy9bdzp8h-6>li:before{content:"\0025cf   "}.lst-kix_m1y38rtq2dr5-7>li:before{content:"\0025cb   "}.lst-kix_m1y38rtq2dr5-8>li:before{content:"\0025a0   "}.lst-kix_55rfy9bdzp8h-7>li:before{content:"\0025cb   "}ol.lst-kix_1mcqazeuqmqh-6.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-6 0}ol.lst-kix_1mcqazeuqmqh-3{list-style-type:none}ol.lst-kix_1mcqazeuqmqh-4{list-style-type:none}.lst-kix_1mcqazeuqmqh-5>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-5}ol.lst-kix_1mcqazeuqmqh-1{list-style-type:none}ol.lst-kix_1mcqazeuqmqh-2{list-style-type:none}.lst-kix_1mcqazeuqmqh-0>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-0}ol.lst-kix_1mcqazeuqmqh-7{list-style-type:none}ol.lst-kix_1mcqazeuqmqh-8{list-style-type:none}ol.lst-kix_1mcqazeuqmqh-5{list-style-type:none}ol.lst-kix_1mcqazeuqmqh-6{list-style-type:none}.lst-kix_1mcqazeuqmqh-4>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-4}.lst-kix_1mcqazeuqmqh-4>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-4,lower-latin) ". "}.lst-kix_1mcqazeuqmqh-6>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-6,decimal) ". "}.lst-kix_1mcqazeuqmqh-1>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-1,lower-latin) ". "}.lst-kix_1mcqazeuqmqh-5>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-5,lower-roman) ". "}ol.lst-kix_1mcqazeuqmqh-2.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-2 0}ol.lst-kix_1mcqazeuqmqh-5.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-5 0}.lst-kix_1mcqazeuqmqh-2>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-2,lower-roman) ". "}ul.lst-kix_55rfy9bdzp8h-0{list-style-type:none}ul.lst-kix_55rfy9bdzp8h-1{list-style-type:none}ul.lst-kix_55rfy9bdzp8h-2{list-style-type:none}li.li-bullet-0:before{margin-left:-18pt;white-space:nowrap;display:inline-block;min-width:18pt}ul.lst-kix_55rfy9bdzp8h-3{list-style-type:none}.lst-kix_1mcqazeuqmqh-3>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-3,decimal) ". "}ul.lst-kix_55rfy9bdzp8h-4{list-style-type:none}.lst-kix_1mcqazeuqmqh-2>li{counter-increment:lst-ctn-kix_1mcqazeuqmqh-2}.lst-kix_1mcqazeuqmqh-0>li:before{content:"" counter(lst-ctn-kix_1mcqazeuqmqh-0,decimal) ". "}ol.lst-kix_1mcqazeuqmqh-8.start{counter-reset:lst-ctn-kix_1mcqazeuqmqh-8 0}ol{margin:0;padding:0}table td,table th{padding:0}.c2{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c14{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c8{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c23{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c24{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c16{padding-top:14pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c17{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c18{padding-top:16pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c20{color:#434343;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c5{padding-top:12pt;padding-bottom:4pt;line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c15{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:12pt;font-family:"Arial";font-style:normal}.c13{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c1{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c7{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c21{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c9{background-color:#ffffff;max-width:468pt;padding:72pt 72pt 72pt 72pt}.c11{margin-left:36pt;padding-left:0pt}.c10{padding:0;margin:0}.c19{color:inherit;text-decoration:inherit}.c22{margin-left:36pt}.c3{font-style:italic}.c6{text-indent:36pt}.c12{font-weight:700}.c4{height:11pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c9 doc-content"><p class="c17 title" id="h.8nr0fgvatfu9"><span class="c24">LLMs: What are they really?</span></p><p class="c1"><span class="c0">LLM(Large Language Model) is a vague term used to describe much of what modern artificial intelligence looks like today. The phrase doesn&rsquo;t do much to stimulate one&rsquo;s imagination and more precise terms such as neural networks, transformers, or hidden layers only add to this confusion. Our goal in this project is to lift the veil that lies over some of the most infamous technology today, understand its inner workings, and build a simple version from the ground up. We used Andrej Karpathy&rsquo;s youtube series &ldquo;Neural Networks: From Zero to Hero&rdquo; to guide our process in demystifying LLMs. </span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.tqh242k7qay7"><span class="c8">Step 1: Micrograd</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In order to understand what an LLM is, we need to start at the basics. Micrograd is an auto gradient engine, and PyTorch, a production level neural network library, has one of these engines implemented, called autograd. Micrograd is a much simpler version of autograd, built on top of scalars instead of autograd&rsquo;s tensors</span><span class="c0">. By using scalars we are essentially taking deep neural networks and breaking them down into the atoms that make them up. </span></p><p class="c1 c6"><span>This exercise is solely for the purpose of really understanding backpropagation. Using tensors allows PyTorch to pack scalars into arrays which when doing computation parallelize the scalar operations. Tensors create efficiency but our scalar implementation here is doing the same thing on a fundamental level that a larger neural network might do. The purpose of m</span><span class="c0">icrograd is to implement a backpropagation algorithm that efficiently evaluates the gradient of some kind of loss function with respect to the weights of a neural network. By tuning the weights of a neural network we can minimize the loss function to improve accuracy of the network. &nbsp; </span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Before we fully explain backpropagation, neural networks, and loss functions, let&#39;s revisit the definition of a derivative.</span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 439.00px; height: 162.88px;"><img alt="" src="images/image10.png" style="width: 624.00px; height: 351.05px; margin-left: -95.00px; margin-top: -93.27px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Recall the limit definition of a derivative shown above. As you make a miniscule change to h and re-evaluate your function, the difference is the change in the function&#39;s value over the period h. If you then divide by h you have found the average increase in the function over the period h. As h approaches 0, you can calculate the instantaneous rate of change at that point.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Looking at the function </span><img src="images/image1.png"><span class="c0">, we can plot it and get the following graph:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c7"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 552.00px; height: 413.00px;"><img alt="" src="images/image7.png" style="width: 552.00px; height: 413.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c7 c4"><span class="c0"></span></p><p class="c1"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now let&#39;s try to evaluate the derivative at the point x = 3. Just by looking at the graph, we know that it has to be positive, since the function is increasing at that point. If we use our limit definition of a derivative with a h value of </span><img src="images/image2.png"><span>, we get something like this: </span><img src="images/image3.png"><span>. This function evaluates to 14.00000009255109, and we can be reasonably sure that the derivative at this point is 14. We can also calculate the derivative exactly by differentiating the function, and finding </span><img src="images/image4.png"><span class="c0">. When we evaluate this function at x = 3, we can see clearly that the derivative is 14. Now how does this help us train neural networks?</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We use derivatives in neural networks to calculate the correct amount that we should change the values within each node in each round of training. This derivative is called the gradient. Each node in our basic neural net affects the outcome a little bit, so we can calculate that small effect using backpropagation.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 169.33px;"><img alt="" src="images/image9.png" style="width: 624.00px; height: 169.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;In this example, we calculated the gradient by figuring out how much each value affects the output when it&rsquo;s changed. For nodes that are being added together, the gradient is always 1 because if we increase the value of that node by 1, we increase the output by 1. This is the case for c and a*b. For nodes that are being multiplied, the gradient is the other factor that it is being multiplied against because that is how much is being added to the final output each time it increases. For example, for b the gradient is the value of a, or 2, because each time b increases by 1 a gets added to the final output one more time. </span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We can calculate the loss using a ground truth. We always have a ground truth when training, which is from a dataset. We can calculate loss using the expected output from the ground truth and the output that the neural net actually spits out. By subtracting the actual output and the expected output, we find the difference between the two. To make the neural network better, we want to minimize the difference. This difference is called loss.</span></p><p class="c1"><span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now, the actual outputs from the nodes don&rsquo;t represent anything right now. They&rsquo;re just numbers. But we want them to represent a probability distribution, so greater numbers mean that a certain output is more likely to happen. We can do this using a layer like the softmax layer, which maps all values to a value between 0 and 1. Then, we randomly pick an output using the resulting distribution and calculate loss based on this.</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 517.94px; height: 517.94px;"><img alt="" src="images/image6.png" style="width: 517.94px; height: 517.94px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;After calculating the gradients, we can perform gradient descent to update the values. We decide a learning rate for our neural network, multiply it against the gradient, and then add that to the old value. This will slowly decrease the loss and move our actual output towards the expected output!</span></p><h1 class="c2" id="h.lgclndlypkkq"><span class="c8">Part 2: Makemore</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Makemore is a character level language model. It can create words by stringing together different characters. Each letter has an associated probability distribution of possible next characters, and we can conduct random sampling on that probability distribution to construct words. Any language model needs training data to learn from, and Makemore uses a dataset of 32,033 names to learn.</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The first step in creating this model is creating the bigrams (two character strings) that the model can learn from. The name Emma can be broken up into 5 bigrams. You are probably wondering &ldquo;Why 5? I count 3 bigrams, {EM, MM, MA}!&rdquo; This intuition is valid, and we thought the same exact thing when we were watching the video. The thing we want to do is add a 27th character to the beginning and end of every word, &ldquo;.&rdquo;! This character represents the start and end of a word. The 4th and 5th bigrams of Emma are {.M} and {A.}. This is important information, as it helps us understand what letters most commonly start and end names.</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;We can create a 27 by 27 array now, with row 0 col 0 being equal to the number of times a follows a in the dataset, row 1 col 0 equalling the number of times a follows b in the dataset, all the way until row 27 col 0 being the number of times a follows &ldquo;.&rdquo; in the dataset. This array looks like the following:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 624.00px;"><img alt="" src="images/image8.png" style="width: 624.00px; height: 624.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;As you can see, certain bigrams like {A.} or {N.} are really common, as the letters a and n most commonly start names. Other bigrams never occur, such as {VM}, as no name in the training set has the letter &ldquo;m&rdquo; following the letter &ldquo;v&rdquo;. </span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Now that we have this 27 by 27 array, we can normalize all the rows, so they each sum to 1. After this step, each row is a probability distribution that we can conduct random sampling on to find the next letter. When we generate some names with these probability distributions above, starting at &ldquo;.&rdquo; and ending when another &ldquo;.&rdquo;, we get the following:</span></p><p class="c7"><span class="c0">cexze.</span></p><p class="c7"><span class="c0">momasurailezitynn.</span></p><p class="c7"><span class="c0">konimittain.</span></p><p class="c7"><span class="c0">llayn.</span></p><p class="c7"><span class="c0">ka.</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;While these strings are somewhat namelike, they are not doing the job we intended. Of course, if we were to randomly sample this distribution we would get much longer, much more chaotic strings. In order to quantify the strength of our model, and its ability to generate realistic names, we can use a loss function!</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Loss functions are used to describe the accuracy of a model&rsquo;s predictions, and values closer to 0 are desired, as the goal is to minimize the loss of the model. The most common approach to finding the loss is taking the negative log likelihood for all of our training data, and averaging it. If we think back to the name Emma, we know from looking at the entry in our table with the bigram {.E} that the chances of E starting a name are 4.78%. We can compute the log of that number, and ln(.0478) equals -3.04. We then do this for every single bigram in our training set, and average out our output, to get an average negative log likelihood of 2.454.<br>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;One important consideration to keep in mind with our model is that if we use data that wasn&rsquo;t in our training set to compute the loss, we could run into a situation where the loss is infinity! If the name &ldquo;Vmishnu&rdquo; was in our training set, our model would assign a probability of 0 to the bigram {VM} occurring, and the natural log of 0 is negative infinity! Even if every other bigram was overwhelmingly close to 0, the average log likelihood will still be negative infinity. One way to remedy this issue is to add 1 to the count of every bigram before normalizing the rows, to ensure that every bigram is accounted for. If we do this to our model and recompute the loss for the name &ldquo;Vmishnu&rdquo;, we get a value of 2.975, which is well above our overall loss of 2.454, but is still a reasonable value.</span></p><p class="c1"><span class="c0">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Another approach to makemore involves neural networks, which do not have this same issue of infinite logs, and we will take you through it right now! Our first step is initializing 27 neurons, each with 27 weights, which we assign randomly. Then, we can take the string emma, and convert it into its bigrams. Once we have said bigrams, we can create two arrays, with one being all the indexes of the first characters in the bigrams, and the other being the indexes of the second characters. We can then convert the first letters into a one-hot encoding, which essentially means converting an int such as 5 into a 1 by 27 array equalling [0, 0, 0, 0, 1, 0, 0, &hellip;, 0]. We can then multiply our 5 one hot encodings of the first letter of the Emma bigrams, a 5 x 27 matrix, by the 27 by 27 matrix equalling the weights of all the neurons. This cross product returns a 5 x 27 matrix of logits. We can then exponentiate these logits and normalize them to have a probability distribution of next character guesses given the input characters in the Emma bigrams. Now, if we compute the loss, and the effect that each neuron&rsquo;s weight has on the loss, we can perform gradient descent on our neurons, and update the weights of our neurons. After 100 training runs, our model&#39;s overall loss becomes 2.49, close to the 2.45 we were able to commute with our previous approach. Our previous approach is not scalable however, as it can&rsquo;t take in more than one character worth of context. The neuron based approach can be scaled to look at multiple characters preceding the character we are trying to predict.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p><h1 class="c2" id="h.s9qtxvgqutex"><span class="c8">Part 3: MLP</span></h1><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">Let&rsquo;s review our makemore model so far. </span></p><ul class="c10 lst-kix_55rfy9bdzp8h-0 start"><li class="c1 c11 li-bullet-0"><span class="c0">We have been using bigrams where one character predicts the next one with a lookup table of counts.</span></li><li class="c1 c11 li-bullet-0"><span class="c0">We looked at only a single previous character and predicted the distribution of the next character in the sequence.</span></li><li class="c1 c11 li-bullet-0"><span class="c0">We looked at the counts of characters and normalized them into a probability distribution.</span></li></ul><p class="c1"><span class="c0">To continue with makemore our makemore model we need to first need to address the issues:</span></p><ul class="c10 lst-kix_m1y38rtq2dr5-0 start"><li class="c1 c11 li-bullet-0"><span class="c0">The predictions were not very good.</span></li><li class="c1 c11 li-bullet-0"><span class="c0">One single character at a time is not enough to create meaningful words, we have only 17 possibilities</span></li><li class="c1 c11 li-bullet-0"><span class="c0">when we have two characters at a time we have 27 x 27 possibilities but it scales as thus as we add more characters</span></li><li class="c1 c11 li-bullet-0"><span class="c0">Our matrix blows up enormously and it would be very computationally costly to calculate</span></li></ul><h2 class="c23" id="h.98acy7hsbz5w"><span class="c14">MLP</span></h2><p class="c1"><span>Stands for </span><span class="c12">M</span><span>ulti </span><span class="c12">L</span><span>ayer </span><span class="c12">P</span><span class="c0">erceptron model</span></p><h3 class="c18" id="h.ej5ya5g61d9n"><span class="c21"><a class="c19" href="https://www.google.com/url?q=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&amp;sa=D&amp;source=editors&amp;ust=1766190672200958&amp;usg=AOvVaw132sQJNPy1zW8CzWLOp5Yt">Bengio et al. 2003</a></span></h3><p class="c1"><span>The model we create follows </span><span class="c21"><a class="c19" href="https://www.google.com/url?q=https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&amp;sa=D&amp;source=editors&amp;ust=1766190672201172&amp;usg=AOvVaw0xe0UGC7lqGiiAxZ8-jvWj">this</a></span><span class="c0">&nbsp;paper.</span></p><h4 class="c16" id="h.gpdkw36dydp1"><span class="c15">Important Note:</span></h4><p class="c1"><span class="c0">Bergio et al. work with word level language models whereas Karpathy&#39;s implementation is a character level model. We can use this approach nonetheless just beware of the difference between our implementation and this paper. I might refer to words as I talk about the paper but characters will be used in our code.</span></p><h4 class="c16" id="h.ej3fixlu0rak"><span class="c15">Proposed Approach:</span></h4><ol class="c10 lst-kix_1mcqazeuqmqh-0 start" start="1"><li class="c1 c11 li-bullet-0"><span>associate each word in the vocabulary a distributed </span><span class="c3">word feature vector</span><span class="c0">&nbsp;(a real valued vector in R^m)</span></li></ol><p class="c1 c4 c22"><span class="c0"></span></p><ol class="c10 lst-kix_1mcqazeuqmqh-0" start="2"><li class="c1 c11 li-bullet-0"><span>express the joint </span><span class="c3">probability function</span><span class="c0">&nbsp;of word sequences in terms of the feature vectors of these words in the sequence, and</span></li></ol><p class="c1 c22 c4"><span class="c0"></span></p><ol class="c10 lst-kix_1mcqazeuqmqh-0" start="3"><li class="c1 c11 li-bullet-0"><span>learn simultaneously the </span><span class="c3">word feature vectors</span><span>&nbsp;and the parameters of that </span><span class="c3">probability function</span></li></ol><h5 class="c5" id="h.fdhogzhsoh1z"><span class="c13">What does this mean?</span></h5><p class="c1"><span class="c0">Basically imagine we have a list of 17,000 words and for each word associate a 30 dimensional feature vector. Each word is embedded in a 30 dimensional space. 30 here corresponds to m in the first note above. It could be 30, 60, or 100, they test through multiple different numbers. These words are tuned using backprop and move around the space. Words with similar meanings will end up in similar parts of the space and opposite for different words.</span></p><h5 class="c5" id="h.m2dudyh6fvp"><span class="c13">Practical example</span></h5><p class="c1"><span class="c0">Suppose you had the phrase:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">A dog was running in a &nbsp;</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">And you wanted to predict what came at the end but you didn&#39;t have this exact phrase within your training set. You would be out of distribution and no reason to suspect what would come next. Maybe you have seen similar phrases such as:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">*The* dog was running in a</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>Maybe our network has learned that </span><span class="c3">a</span><span>&nbsp;and </span><span class="c3">the</span><span>&nbsp;are similar and interchangeable. Thus the network has placed </span><span class="c3">a</span><span>&nbsp;and </span><span class="c3">the</span><span>&nbsp;close to each other within the space of their embeddings. A similar idea is with </span><span class="c3">cat</span><span>&nbsp;and </span><span class="c3">dog</span><span class="c0">. We can then apply these similar findings to generalize novel scenarios. Say we had the following phrase in our training data:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">The cat was walking in the bedroom</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">We could infer the full sentence from before could be:</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span class="c0">A dog was running in a room</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1"><span>Our network might have learned that </span><span class="c3">a</span><span>&nbsp;and </span><span class="c3">the</span><span>&nbsp;are interchangeable as well as </span><span class="c3">cat</span><span>&nbsp;and </span><span class="c3">dog</span><span>, </span><span class="c3">running</span><span>&nbsp;and </span><span class="c3">walking</span><span>, and </span><span class="c3">room</span><span>&nbsp;and </span><span class="c3">bedroom</span><span class="c0">. Therefore we can infer the end of the sentence.</span></p><h4 class="c16" id="h.v8rajelfxojm"><span class="c15">Neural Network Diagram</span></h4><p class="c1"><span class="c0">The neural network diagram below is an example of the MLP model we will implement. </span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 489.33px;"><img alt="" src="images/image5.png" style="width: 624.00px; height: 489.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c0">Let&#39;s break this diagram up into the three layers.</span></p><h5 class="c5" id="h.rqvidws5v6ub"><span class="c13">Layer One</span></h5><p class="c1"><span class="c0">In this example we are taking three previous words and attempting to predict the fourth word in the sequence. The three previous words are represented by their indexes within a look up table C as the green boxes at the bottom of the diagram and the output at the top will be used to give the fourth word. If we use our example from before where we have 17,000 words then the index will be an integer 0-16,999 and we are indexing into a matrix C that is 17,000 by say 30(could be 50, 100 see above). Every index is plucking out a row from the lookup table and getting that 30 dimensional embedding vector for that word.</span></p><h5 class="c5" id="h.ro023n2sg6aq"><span class="c13">Layer Two</span></h5><p class="c1"><span class="c0">Next is the hidden layer, tanh. It is a hyperparameter that can be as large or small as you like. Say there were 100 neurons in this layer, they would be connected to all of the 90 dimensions that make up the three words from before.</span></p><h5 class="c5" id="h.lhfpymup9nzd"><span class="c13">Layer Three</span></h5><p class="c1"><span class="c0">Then the final layer is a softmax layer that contains all the possibilities of the 17,000 words that could come next. Thus it is a 17,000 logit layer that is connected to each of the 100 neurons from the hidden layer. This softmax layer is normalized and adds up to 1 that is therefore a probability distribution of the next word to come in the sequence.</span></p><p class="c1 c4"><span class="c0"></span></p><p class="c1 c4"><span class="c0"></span></p></body></html>